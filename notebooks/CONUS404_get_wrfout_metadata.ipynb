{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab5e200",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ebd4dc",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Name of the metadata output file to write\n",
    "metadata_output_file = '<some_path>/wrfout_metadata.csv'\n",
    "\n",
    "# The initial variable attributes are read from one of the wrfout_* model output files\n",
    "wrfout_file = '<some_path>/kyoko/OUTPUT/WY1996/wrfout_d01_1996-10-01_00:00:00'\n",
    "\n",
    "base_dir = '<some_path>/02_conus404_metadata/wrfout_overrides'\n",
    "\n",
    "# Directory containing the overrides files\n",
    "overrides_dir = base_dir\n",
    "\n",
    "wrf_long_name_override = f'{overrides_dir}/wrfout_long_name_overrides.txt'\n",
    "wrf_units_override = f'{overrides_dir}/wrfout_units_overrides.txt'\n",
    "wrf_valid_range_overrides = f'{overrides_dir}/wrfout_valid_range_overrides.txt'\n",
    "wrf_flag_values_overrides = f'{overrides_dir}/wrfout_flag_values_overrides.txt'\n",
    "wrf_flag_meanings_overrides = f'{overrides_dir}/wrfout_flag_meanings_overrides.txt'\n",
    "wrf_notes_overrides = f'{overrides_dir}/wrfout_notes_overrides.txt'\n",
    "wrf_scale_factor_overrides = f'{overrides_dir}/wrfout_scale_factor_overrides.txt'\n",
    "\n",
    "# File containing mappings of words in the wrfout_* attributes to what they should be changed to\n",
    "wrf_wordmap_file = f'{overrides_dir}/wrfout_wordmap.csv'\n",
    "\n",
    "\n",
    "# Variables that are integrated over 60 minutes per hourly timestep\n",
    "vars_60min_accum = ['ACDEWC', 'ACDRIPR', 'ACDRIPS', 'ACECAN', 'ACEDIR', 'ACETLSM', 'ACETRAN',\n",
    "                    'ACEVAC', 'ACEVB', 'ACEVC', 'ACEVG', 'ACFROC', 'ACFRZC', 'ACGHB', 'ACGHFLSM',\n",
    "                    'ACGHV', 'ACINTR', 'ACINTS', 'ACIRB', 'ACIRC', 'ACIRG', 'ACLHFLSM', 'ACLWDNLSM',\n",
    "                    'ACLWUPLSM', 'ACMELTC', 'ACPAHB', 'ACPAHG', 'ACPAHLSM', 'ACPAHV', 'ACPONDING',\n",
    "                    'ACQLAT', 'ACQRF', 'ACRAINLSM', 'ACRAINSNOW', 'ACRUNSB', 'ACRUNSF', 'ACSAGB',\n",
    "                    'ACSAGV', 'ACSAV', 'ACSHB', 'ACSHC', 'ACSHFLSM', 'ACSHG', 'ACSNBOT', 'ACSNFRO',\n",
    "                    'ACSNOWLSM', 'ACSNSUB', 'ACSUBC', 'ACSWDNLSM', 'ACSWUPLSM', 'ACTHROR', 'ACTHROS',\n",
    "                    'ACTR', 'GRAUPEL_ACC_NC', 'PREC_ACC_C', 'PREC_ACC_NC', 'SNOW_ACC_NC']\n",
    "\n",
    "# Variables that are accumulated from model start\n",
    "vars_model_accum = ['ACGRDFLX', 'ACHFX', 'ACLHF',\n",
    "                    'ACSNOM',\n",
    "                    'GRAUPELNC', 'HAILNC',\n",
    "                    'I_ACLWDNB', 'I_ACLWDNBC', 'I_ACLWDNT', 'I_ACLWDNTC', 'I_ACLWUPB', \n",
    "                    'I_ACLWUPBC', 'I_ACLWUPT', 'I_ACLWUPTC', 'I_ACSWDNB', 'I_ACSWDNBC', \n",
    "                    'I_ACSWDNT', 'I_ACSWDNTC', 'I_ACSWUPB', 'I_ACSWUPBC', 'I_ACSWUPT', \n",
    "                    'I_ACSWUPTC', 'I_RAINC', 'I_RAINNC',\n",
    "                    'QRFS', 'QSLAT', 'QSPRINGS',\n",
    "                    'RAINSH', 'RECH', 'SNOWNC']\n",
    "\n",
    "vars_bucket_J_accum = ['ACLWDNB', 'ACLWDNBC', 'ACLWDNT', 'ACLWDNTC', 'ACLWUPB', 'ACLWUPBC', 'ACLWUPT', \n",
    "                       'ACLWUPTC', 'ACSWDNB', 'ACSWDNBC', 'ACSWDNT', 'ACSWDNTC', 'ACSWUPB', 'ACSWUPBC', \n",
    "                       'ACSWUPT', 'ACSWUPTC']\n",
    "\n",
    "vars_bucket_mm_accum = ['RAINC', 'RAINNC']\n",
    "\n",
    "print(f'{len(vars_60min_accum)=}')\n",
    "print(f'{len(vars_model_accum)=}')\n",
    "print(f'{len(vars_bucket_mm_accum)=}')\n",
    "print(f'{len(vars_bucket_J_accum)=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240d8ba6",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def read_override_file(filename):\n",
    "    # Read override file\n",
    "    fhdl = open(filename, 'r', encoding='ascii')\n",
    "    rawdata = fhdl.read().splitlines()\n",
    "    fhdl.close()\n",
    "\n",
    "    it = iter(rawdata)\n",
    "    next(it)   # Skip header\n",
    "\n",
    "    override_map = {}\n",
    "    for row in it:\n",
    "        flds = row.split('\\t')\n",
    "        override_map[flds[0]] = flds[1]\n",
    "        # print(flds)  \n",
    "    return override_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36422612",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566b6b11",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Read word map file for processing the description strings\n",
    "fhdl = open(wrf_wordmap_file, 'r', encoding='ascii')\n",
    "rawdata = fhdl.read().splitlines()\n",
    "fhdl.close()\n",
    "\n",
    "it = iter(rawdata)\n",
    "next(it)   # Skip header\n",
    "\n",
    "word_map = {}\n",
    "for row in it:\n",
    "    flds = row.split('\\t')\n",
    "    if len(flds[2]) != 0:\n",
    "        word_map[flds[0].replace('\"', '')] = flds[2].replace('\"', '')\n",
    "    # print(flds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed521cf",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Read long_name override file \n",
    "long_name_map = read_override_file(wrf_long_name_override)\n",
    "\n",
    "# wrf_valid_range_overrides\n",
    "valid_range_map = read_override_file(wrf_valid_range_overrides)\n",
    "\n",
    "# wrf_flag_values_overrides\n",
    "flag_values_map = read_override_file(wrf_flag_values_overrides)\n",
    "\n",
    "# wrf_flag_meanings_overrides\n",
    "flag_meanings_map = read_override_file(wrf_flag_meanings_overrides)\n",
    "\n",
    "# wrf_notes_overrides\n",
    "notes_map = read_override_file(wrf_notes_overrides)\n",
    "\n",
    "# wrf_scale_factor_overrides\n",
    "scale_factor_map = read_override_file(wrf_scale_factor_overrides)\n",
    "\n",
    "# Read units override file\n",
    "units_map = read_override_file(wrf_units_override)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a58888",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Read dimensions, variables, and attributes from a single wrfout file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41977973",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = xr.open_dataset(wrfout_file, decode_coords=False, chunks={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e07e23e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "attr_cnt = Counter()\n",
    "word_cnt = Counter()\n",
    "\n",
    "wrfout_vars = {}\n",
    "\n",
    "for vv in list(df.keys()):\n",
    "    cvar = df[vv]\n",
    "    wrfout_vars[vv] = {}\n",
    "    \n",
    "    for cattr, val in cvar.attrs.items():\n",
    "        if cattr in ['description', 'units', 'coordinates']:\n",
    "            attr_cnt[cattr] += 1\n",
    "            \n",
    "            if cattr == 'units':\n",
    "                if vv in units_map:\n",
    "                    # Units are overidden\n",
    "                    wrfout_vars[vv][cattr] = units_map[vv]\n",
    "                else:\n",
    "                    wrfout_vars[vv][cattr] = val\n",
    "            elif cattr == 'description':\n",
    "                # Copy the original description\n",
    "                wrfout_vars[vv][cattr] = val\n",
    "                \n",
    "                # Add a long_name attribute\n",
    "                if vv in long_name_map:\n",
    "                    # long_name is overidden\n",
    "                    wrfout_vars[vv]['long_name'] = long_name_map[vv]\n",
    "                else:\n",
    "                    # Construct long_name from the word map\n",
    "                    new_val = []\n",
    "                    for ww in val.split(' '):\n",
    "                        if ww in word_map:\n",
    "                            new_val.append(word_map[ww])\n",
    "                        else:\n",
    "                            new_val.append(ww)\n",
    "                        word_cnt[ww] += 1\n",
    "\n",
    "                    # result = string[0].upper() + string[1:]\n",
    "                    outstr = ' '.join(new_val)\n",
    "\n",
    "                    if len(outstr) > 0:\n",
    "                        outstr = outstr[0].upper() + outstr[1:]\n",
    "                    wrfout_vars[vv]['long_name'] = outstr\n",
    "            else:\n",
    "                # Just copy other attributes\n",
    "                wrfout_vars[vv][cattr] = val\n",
    "                \n",
    "    wrfout_vars[vv]['datatype'] = cvar.encoding['dtype'].name\n",
    "    wrfout_vars[vv]['dimensions'] = ' '.join(cvar.dims)\n",
    "    \n",
    "    if vv == 'XTIME':\n",
    "        # Units doesn't exist for XTIME so we'll create it\n",
    "        wrfout_vars[vv]['units'] = units_map[vv]\n",
    "        \n",
    "    if vv == 'Times':\n",
    "        # The Times variable is missing any sort of description\n",
    "        wrfout_vars[vv]['long_name'] = long_name_map[vv]\n",
    "        \n",
    "    if vv in valid_range_map:\n",
    "        wrfout_vars[vv]['valid_range'] = valid_range_map[vv]\n",
    "    if vv in flag_values_map:\n",
    "        wrfout_vars[vv]['flag_values'] = flag_values_map[vv]\n",
    "    if vv in flag_meanings_map:\n",
    "        wrfout_vars[vv]['flag_meanings'] = flag_meanings_map[vv]\n",
    "    if vv in notes_map:\n",
    "        wrfout_vars[vv]['notes'] = notes_map[vv]\n",
    "    if vv in scale_factor_map:\n",
    "        wrfout_vars[vv]['scale_factor'] = scale_factor_map[vv]\n",
    "        \n",
    "    if vv in vars_60min_accum:\n",
    "        # Add accumulated and integration field\n",
    "        wrfout_vars[vv]['accumulated'] = True\n",
    "        wrfout_vars[vv]['integration_length'] = 'accumulated over prior 60 minutes'\n",
    "        \n",
    "        # Add a cell_methods field\n",
    "        # wrfout_vars[vv]['cell_methods'] = 'XTIME: sum (interval: 1 minute)'\n",
    "    elif vv in vars_model_accum:\n",
    "        # Add accumulated and integration field\n",
    "        wrfout_vars[vv]['accumulated'] = True\n",
    "        wrfout_vars[vv]['integration_length'] = 'accumulated since 1979-10-01 00:00:00'\n",
    "    elif vv in vars_bucket_J_accum:\n",
    "        wrfout_vars[vv]['accumulated'] = True\n",
    "        wrfout_vars[vv]['integration_length'] = 'accumulated since last bucket_J (1.0e9 J m-2) reset'\n",
    "    elif vv in vars_bucket_mm_accum:\n",
    "        wrfout_vars[vv]['accumulated'] = True\n",
    "        wrfout_vars[vv]['integration_length'] = 'accumulated since last bucket_mm (100 mm) reset'\n",
    "    else:\n",
    "        wrfout_vars[vv]['accumulated'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c03ddae",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "attr_cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bb0aac",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Create a dataframe of the new metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd913c2",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "out_df = pd.DataFrame(wrfout_vars).transpose()\n",
    "out_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496eec8a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Write the new metadata to a CSV file\n",
    "out_df.sort_index().to_csv(metadata_output_file, sep='\\t', index_label = 'varname', \n",
    "                           columns=['long_name', 'accumulated', 'integration_length', \n",
    "                                    'description', 'notes', 'units', 'scale_factor', 'valid_range', \n",
    "                                    'flag_values', 'flag_meanings', 'dimensions', 'coordinates', 'datatype'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bba7b0e-556c-4493-86ab-15d8b4cd18b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pangeo]",
   "language": "python",
   "name": "conda-env-pangeo-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
