{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4141ea8f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e3c57a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Name of the metadata output file to write\n",
    "metadata_output_file = '/home/pnorton/tmp/auxhist24_metadata.csv'\n",
    "\n",
    "# The initial variable attributes are read from one of the wrfxtrm_* model output files\n",
    "auxhist24_file = f'/caldera/projects/usgs/water/impd/wrf-conus404/kyoko/OUTPUT/WY1996/auxhist24_d01_1996-10-01_00:00:00'\n",
    "\n",
    "base_dir = '/home/pnorton/notebooks/02_conus404_metadata/auxhist24_overrides'\n",
    "\n",
    "# Directory containing the overrides files\n",
    "overrides_dir = base_dir\n",
    "\n",
    "wrf_wordmap_file = f'{overrides_dir}/auxhist24_wordmap.csv'\n",
    "wrf_long_name_override = f'{overrides_dir}/auxhist24_long_name_overrides.txt'\n",
    "wrf_units_override = f'{overrides_dir}/auxhist24_units_overrides.txt'\n",
    "wrf_notes_overrides = f'{overrides_dir}/auxhist24_notes_overrides.txt'\n",
    "wrf_scale_factor_overrides = f'{overrides_dir}/auxhist24_scale_factor_overrides.txt'\n",
    "\n",
    "# Variables that are integrated over 60 minutes per hourly timestep\n",
    "vars_60min_accum = ['PREC_ACC_NC']\n",
    "vars_model_accum = ['I_RAINNC']\n",
    "vars_bucket_mm_accum = ['RAINNC']\n",
    "\n",
    "print(f'{len(vars_60min_accum)=}')\n",
    "print(f'{len(vars_model_accum)=}')\n",
    "print(f'{len(vars_bucket_mm_accum)=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae725793",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def read_override_file(filename):\n",
    "    # Read override file\n",
    "    fhdl = open(filename, 'r', encoding='ascii')\n",
    "    rawdata = fhdl.read().splitlines()\n",
    "    fhdl.close()\n",
    "\n",
    "    it = iter(rawdata)\n",
    "    next(it)   # Skip header\n",
    "\n",
    "    override_map = {}\n",
    "    for row in it:\n",
    "        flds = row.split('\\t')\n",
    "        override_map[flds[0]] = flds[1]\n",
    "        # print(flds)  \n",
    "    return override_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c001178",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Read word map file for processing the description strings\n",
    "fhdl = open(wrf_wordmap_file, 'r', encoding='ascii')\n",
    "rawdata = fhdl.read().splitlines()\n",
    "fhdl.close()\n",
    "\n",
    "it = iter(rawdata)\n",
    "next(it)   # Skip header\n",
    "\n",
    "word_map = {}\n",
    "for row in it:\n",
    "    flds = row.split('\\t')\n",
    "    if len(flds[2]) != 0:\n",
    "        word_map[flds[0].replace('\"', '')] = flds[2].replace('\"', '')\n",
    "    # print(flds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9464cf2e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "word_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb20564",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Read long_name override file \n",
    "long_name_map = read_override_file(wrf_long_name_override)\n",
    "\n",
    "# wrf_notes_overrides\n",
    "notes_map = read_override_file(wrf_notes_overrides)\n",
    "\n",
    "# wrf_scale_factor_overrides\n",
    "scale_factor_map = read_override_file(wrf_scale_factor_overrides)\n",
    "\n",
    "# Read units override file\n",
    "units_map = read_override_file(wrf_units_override)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d778b6-808e-4831-9fc6-268ddcb4cbad",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "valid_range_map = {}\n",
    "flag_values_map = {}\n",
    "flag_meanings_map = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ac03c2",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Read dimensions, variables, and attributes from a single wrfout file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9533ca7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = xr.open_dataset(auxhist24_file, decode_coords=False, chunks={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61768ae0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df.Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3180a5f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "attr_cnt = Counter()\n",
    "word_cnt = Counter()\n",
    "\n",
    "wrfout_vars = {}\n",
    "\n",
    "for vv in list(df.keys()):\n",
    "    cvar = df[vv]\n",
    "    wrfout_vars[vv] = {}\n",
    "    \n",
    "    for cattr, val in cvar.attrs.items():\n",
    "        if cattr in ['description', 'units', 'coordinates']:\n",
    "            attr_cnt[cattr] += 1\n",
    "            \n",
    "            if cattr == 'units':\n",
    "                if vv in units_map:\n",
    "                    # Units are overidden\n",
    "                    wrfout_vars[vv][cattr] = units_map[vv]\n",
    "                else:\n",
    "                    wrfout_vars[vv][cattr] = val\n",
    "            elif cattr == 'description':\n",
    "                # Copy the original description\n",
    "                wrfout_vars[vv][cattr] = val\n",
    "                \n",
    "                # Add a long_name attribute\n",
    "                if vv in long_name_map:\n",
    "                    # long_name is overidden\n",
    "                    wrfout_vars[vv]['long_name'] = long_name_map[vv]\n",
    "                else:\n",
    "                    # Construct long_name from the word map\n",
    "                    new_val = []\n",
    "                    for ww in val.split(' '):\n",
    "                        if ww in word_map:\n",
    "                            new_val.append(word_map[ww])\n",
    "                        else:\n",
    "                            new_val.append(ww)\n",
    "                        word_cnt[ww] += 1\n",
    "\n",
    "                    # result = string[0].upper() + string[1:]\n",
    "                    outstr = ' '.join(new_val)\n",
    "\n",
    "                    if len(outstr) > 0:\n",
    "                        outstr = outstr[0].upper() + outstr[1:]\n",
    "                    wrfout_vars[vv]['long_name'] = outstr\n",
    "            else:\n",
    "                # Just copy other attributes\n",
    "                wrfout_vars[vv][cattr] = val\n",
    "                \n",
    "    wrfout_vars[vv]['datatype'] = cvar.encoding['dtype'].name\n",
    "    wrfout_vars[vv]['dimensions'] = ' '.join(cvar.dims)\n",
    "    \n",
    "    if vv == 'XTIME':\n",
    "        # Units doesn't exist for XTIME so we'll create it\n",
    "        wrfout_vars[vv]['units'] = units_map[vv]\n",
    "        \n",
    "    if vv == 'Times':\n",
    "        # The Times variable is missing any sort of description\n",
    "        wrfout_vars[vv]['long_name'] = long_name_map[vv]\n",
    "        \n",
    "    if vv in valid_range_map:\n",
    "        wrfout_vars[vv]['valid_range'] = valid_range_map[vv]\n",
    "    if vv in flag_values_map:\n",
    "        wrfout_vars[vv]['flag_values'] = flag_values_map[vv]\n",
    "    if vv in flag_meanings_map:\n",
    "        wrfout_vars[vv]['flag_meanings'] = flag_meanings_map[vv]\n",
    "    if vv in notes_map:\n",
    "        wrfout_vars[vv]['notes'] = notes_map[vv]\n",
    "    if vv in scale_factor_map:\n",
    "        wrfout_vars[vv]['scale_factor'] = scale_factor_map[vv]\n",
    "        \n",
    "    if vv in vars_60min_accum:\n",
    "        # Add accumulated and integration field\n",
    "        wrfout_vars[vv]['accumulated'] = True\n",
    "        \n",
    "        # For daily variables the 60-min accumulation is not valid\n",
    "        # wrfout_vars[vv]['integration_length'] = 'accumulated over prior 60 minutes'\n",
    "        wrfout_vars[vv]['integration_length'] = 'accumulated since last top-of-hour'\n",
    "    elif vv in vars_model_accum:\n",
    "        # Add accumulated and integration field\n",
    "        wrfout_vars[vv]['accumulated'] = True\n",
    "        wrfout_vars[vv]['integration_length'] = 'accumulated since 1979-10-01 00:00:00'\n",
    "    elif vv in vars_bucket_mm_accum:\n",
    "        wrfout_vars[vv]['accumulated'] = True\n",
    "        wrfout_vars[vv]['integration_length'] = 'accumulated since last bucket_mm (100 mm) reset'\n",
    "    else:\n",
    "        wrfout_vars[vv]['accumulated'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311e9250",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "attr_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df2b7f0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "out_df = pd.DataFrame(wrfout_vars).transpose()\n",
    "out_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef88531-effc-4c43-b660-e7833046048b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "out_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659fbadf",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "out_df.sort_index().to_csv(metadata_output_file, sep='\\t', index_label = 'varname', \n",
    "                           columns=['long_name', 'accumulated', 'integration_length',\n",
    "                                    'description', 'notes', 'units', 'scale_factor', \n",
    "                                    'dimensions', 'coordinates', 'datatype'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f40b3c8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "59ba92e9",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Don't run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0e4fe5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "word_df = pd.DataFrame(word_cnt, index=[0]).transpose()\n",
    "word_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04dd7024",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#word_df.to_csv('wrfout_words.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea3a6c2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99228dbd",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fhdl = open('wrfout_words.txt', 'r', encoding='ascii')\n",
    "rawdata = fhdl.read().splitlines()\n",
    "fhdl.close()\n",
    "\n",
    "it = iter(rawdata)\n",
    "next(it)   # Skip header\n",
    "\n",
    "word_map = {}\n",
    "for row in it:\n",
    "    flds = row.split('\\t')\n",
    "    if len(flds[2]) != 0:\n",
    "        word_map[flds[0].replace('\"', '')] = flds[2].replace('\"', '')\n",
    "    print(flds)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24552572",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "word_map['LATITUDE,']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3942ece2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "len(flds[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7328b5bf",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "89f00d8e-8681-412a-b320-75ca46d00bb5",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Create word map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc8c809",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = xr.open_dataset(auxhist24_file, decode_coords=False, engine='netcdf4', chunks={})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a880fc",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "attr_cnt = Counter()\n",
    "word_cnt = Counter()\n",
    "\n",
    "wrfout_vars = {}\n",
    "word_map = {}\n",
    "\n",
    "for vv in list(df.keys()):\n",
    "    cvar = df[vv]\n",
    "    wrfout_vars[vv] = {}\n",
    "    \n",
    "    for cattr, val in cvar.attrs.items():\n",
    "        if cattr in ['description', 'units', 'coordinates']:\n",
    "            attr_cnt[cattr] += 1\n",
    "            wrfout_vars[vv][cattr] = val\n",
    "            \n",
    "            if cattr == 'description':\n",
    "                new_val = []\n",
    "                for ww in val.split(' '):\n",
    "                    if ww in word_map:\n",
    "                        new_val.append(word_map[ww])\n",
    "                    else:\n",
    "                        new_val.append(ww)\n",
    "                    word_cnt[ww] += 1\n",
    "                    \n",
    "#                 result = string[0].upper() + string[1:]\n",
    "                outstr = ' '.join(new_val)\n",
    "    \n",
    "                if len(outstr) > 0:\n",
    "                    outstr = outstr[0].upper() + outstr[1:]\n",
    "                wrfout_vars[vv]['description_new'] = outstr\n",
    "    \n",
    "    wrfout_vars[vv]['datatype'] = cvar.encoding['dtype'].name\n",
    "    wrfout_vars[vv]['dimensions'] = ' '.join(cvar.dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f103fad5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "word_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c83bc6c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "wordmap_df = pd.DataFrame(word_cnt, index=[0]).transpose()\n",
    "wordmap_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390b339f-2b10-4bf6-ae63-6d922d327987",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "wordmap_df.to_csv(f'{overrides_dir}/auxhist24_wordmap.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fa1bd4-f881-4c23-bf3d-636c55cd015c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733d5af4-4337-47a4-a8c1-48c2ef087132",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5155de82-966b-41b3-9313-008e016d3181",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bff008-26ba-4eb3-aa32-d33b5839eca6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f648bb00-413e-46b8-a4ae-eec49912959b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pangeo]",
   "language": "python",
   "name": "conda-env-pangeo-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
